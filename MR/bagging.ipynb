{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79ec6bb",
   "metadata": {},
   "source": [
    "## INFOH515 Pyspark code\n",
    "## Author: Gianluca Bontempi\n",
    "## Pyspark implementation of the bagging algorithm in the INFOH515 slides \"Map-reduce analytics\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2763623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/17 16:56:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pwd\n",
    "import getpass\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# create an instance of SparkSession\n",
    "spark=SparkSession.builder.appName('s.com').getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce59fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.61584486,  1.40410217,  1.84505492,  1.27428404, -1.15801998,\n",
       "        -0.77114336,  0.34615021, -1.58390158, -0.47663391,  0.67023779,\n",
       "        -0.6931159 ]),\n",
       " array([ 3.93318275, -1.2784718 ,  1.22773916,  0.6629516 ,  0.26926281,\n",
       "         0.30634199, -0.15450905,  0.77881069, -0.09776944,  0.39046338,\n",
       "        -0.55891276])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1225)   \n",
    "\n",
    "\n",
    "Ntr=1500\n",
    "Nts=100\n",
    "N=Ntr+Nts\n",
    "n=10\n",
    "npartitions=5\n",
    "\n",
    "\n",
    "X= np.random.normal(loc=0, scale=1, size=N * n).reshape(N, n)\n",
    "Y=2+(X[:,0]**2)-3*X[:,7]+np.random.normal(loc=0, scale=0.1, size=N )\n",
    "Y=Y.reshape(N, 1)\n",
    "\n",
    "\n",
    "Xtr=X[:Ntr,]\n",
    "Ytr=Y[:Ntr,]\n",
    "Xts=X[Ntr:,]\n",
    "Yts=Y[Ntr:,]\n",
    "Ytr.shape=(Ntr,1)\n",
    "YX=np.hstack((Ytr,Xtr))\n",
    "vYts=np.var(Yts)\n",
    "YXrdd=sc.parallelize(YX,npartitions)\n",
    "\n",
    "YXrdd.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22d2ff",
   "metadata": {},
   "source": [
    "The dataset is split into npartitions and the same learning algorithm is applied to each partition.\n",
    "The test error of the learning algorithm trained on the first partition is compared with the test error of the averaging approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe6fae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)>                                                          (0 + 5) / 5]\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def rddCreateModels(iterator,mod):\n",
    "\n",
    "    data=np.array(list(iterator))\n",
    "    print(data.shape)\n",
    "    X = data[:, 1:]\n",
    "    Y=data[:, 0]\n",
    "    m=mod.fit(X,Y)  \n",
    "    return [m]\n",
    "\n",
    "def rddUseModel(iterator,Xts):\n",
    "    rfit=list(iterator)[0]\n",
    "    #return [pow(Yts-rfit.predict(Xts),2)]\n",
    "    return [rfit.predict(Xts)]\n",
    "\n",
    "def rddApplyMean(D,axis=0):\n",
    "    if (axis==0): # column\n",
    "        N=D.count()\n",
    "        return(D.reduce(lambda x,y:x+y)/N)\n",
    "\n",
    "    if (axis==1): #row\n",
    "        return(rddArr(D.map(lambda x:mean(x))))\n",
    "    \n",
    "nT=1000\n",
    "mD=15\n",
    "ncores=3\n",
    "\n",
    "mregr0= RandomForestRegressor(n_estimators=nT,max_depth=mD,n_jobs=ncores)\n",
    "M=YXrdd.mapPartitions(lambda x: rddCreateModels(x,mregr0))\n",
    "M.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa5a43",
   "metadata": {},
   "source": [
    "## Prediction using only the first partition as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec02c82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(300, 11)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05293836183324284"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yhat1=M.mapPartitions(lambda x: rddUseModel(x,Xts)).first()\n",
    "errhat=Yts.reshape(Nts,1)-Yhat1.reshape(Nts,1)\n",
    "\n",
    "NMSE1=np.mean(pow(errhat,2))/vYts\n",
    "\n",
    "print(\"NMSE1=\", NMSE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875174c",
   "metadata": {},
   "source": [
    "## Prediction by averaging over all the models (one per partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d329033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)                                                                       \n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "(300, 11)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.042068564587990194"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yhats=rddApplyMean(M.mapPartitions(lambda x: rddUseModel(x,Xts)))\n",
    "errhat=Yts.reshape(Nts,1)-Yhats.reshape(Nts,1)\n",
    "NMSEav=np.mean(pow(errhat,2))/vYts\n",
    "\n",
    "print(\"NMSEav=\", NMSEav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cebd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
